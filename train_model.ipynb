{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from model import TransformerClassifier\n",
    "from datetime import datetime\n",
    "from data import MyDataset, TextDataset\n",
    "from utils import logprint, create_unique_folder, plot_and_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created at: output/roberta_8\n"
     ]
    }
   ],
   "source": [
    "config = SimpleNamespace()\n",
    "\n",
    "# transformer\n",
    "config.model_type = 'bert' # [bert, roberta, xlnet]\n",
    "config.freeze_layer = 8     # the first n number of transformer layers to freeze\n",
    "config.freeze_embedding = True  # whether to freeze the word embedding layer in the transformer\n",
    "config.freeze_pool = True       # for bert/robert only. whether to freeze the pooling layer \n",
    "config.freeze_summary = True    # for xlnet only. whether to freeze the summary layer\n",
    "\n",
    "# cls\n",
    "config.input_size = 768          # the input dimension to the classifier, equal to the output of transformer's logit\n",
    "config.hidden_layers = []         # a list of int. The hidden layers of the cls, e.g. [256, 128] means two hidden layers of 256 and 128 respectively\n",
    "config.num_classes = 8          # number of classes to classify\n",
    "\n",
    "# data\n",
    "config.dataset_folder = 'USs/user_stories_score_full.csv'\n",
    "config.train_batch_size = 32\n",
    "config.test_batch_size = 32\n",
    "\n",
    "# train\n",
    "config.epochs = 2\n",
    "config.lr = 2e-5\n",
    "config.weight_decay = 0.01\n",
    "config.log_freq = 10        # log/print frequency\n",
    "config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# output\n",
    "config.output_folder = 'output/roberta'\n",
    "config.output_folder = create_unique_folder(config.output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### 1. Raw text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = MyDataset(config.dataset_folder)\n",
    "\n",
    "# 分出训练集和测试集\n",
    "US_train, US_test, label_train, label_test = train_test_split(\n",
    "    dataset.US, dataset.label, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 创建训练集和测试集的 Dataset 实例\n",
    "train_dataset = MyDataset(None)\n",
    "train_dataset.US = US_train\n",
    "train_dataset.label = label_train\n",
    "\n",
    "test_dataset = MyDataset(None)\n",
    "test_dataset.US = US_test\n",
    "test_dataset.label = label_test\n",
    "\n",
    "# 将数据集的US处理成列表 然后进行tokenizer\n",
    "train_text = train_dataset.US.tolist()\n",
    "test_text = test_dataset.US.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tokenizer\n",
    "\n",
    "if config.model_type == 'bert':\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # out: ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "elif config.model_type == 'roberta':\n",
    "    from transformers import RobertaTokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')    # out: ['input_ids', 'attention_mask']\n",
    "elif config.model_type == 'xlnet':\n",
    "    from transformers import XLNetTokenizer\n",
    "    tokenizer = XLNetTokenizer.from_pretrained('xlnet/xlnet-base-cased')  # out: ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input attributes dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "tensor(29589)\n"
     ]
    }
   ],
   "source": [
    "# tokenize features\n",
    "\n",
    "train_features = tokenizer(text=train_text,\n",
    "                           add_special_tokens=True,\n",
    "                           padding='max_length',\n",
    "                           truncation=True,\n",
    "                           max_length=128,\n",
    "                           return_tensors='pt')\n",
    "\n",
    "test_features = tokenizer(text=test_text,\n",
    "                          add_special_tokens=True,\n",
    "                          padding='max_length',\n",
    "                          truncation=True,\n",
    "                          max_length=128,\n",
    "                          return_tensors='pt')\n",
    "\n",
    "print('input attributes', train_features.keys())\n",
    "print(train_features['input_ids'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape: torch.Size([1433])\n"
     ]
    }
   ],
   "source": [
    "# vectorize labels\n",
    "\n",
    "# 把训练和测试的label格式转换成tensor，因为tensor()不能接受str类型的list，所以使用OneHot转换成数字类型\n",
    "labelEncoder= LabelEncoder()\n",
    "train_labels = torch.tensor(labelEncoder.fit_transform(train_dataset.label.values))\n",
    "test_labels = torch.tensor(labelEncoder.fit_transform(test_dataset.label.values))\n",
    "\n",
    "print('labels shape:', train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TextDataset(train_features, train_labels)\n",
    "test_data = TextDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=config.train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=config.test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "froze encoder layer 0\n",
      "froze encoder layer 1\n",
      "froze encoder layer 2\n",
      "froze encoder layer 3\n",
      "froze encoder layer 4\n",
      "froze encoder layer 5\n",
      "froze encoder layer 6\n",
      "froze encoder layer 7\n",
      "froze embedding\n",
      "froze pooler\n"
     ]
    }
   ],
   "source": [
    "# init transformer backbone and freeze weights\n",
    "\n",
    "if config.model_type == 'bert':\n",
    "    from transformers import BertModel\n",
    "    from model import freeze_bert_weights\n",
    "    backbone = BertModel.from_pretrained('bert-base-uncased')\n",
    "    print(backbone)\n",
    "    freeze_bert_weights(backbone, config.freeze_layer, config.freeze_embedding, config.freeze_pool)\n",
    "elif config.model_type == 'roberta':\n",
    "    from transformers import BertModel\n",
    "    from model import freeze_bert_weights\n",
    "    backbone = BertModel.from_pretrained('roberta-base')\n",
    "    print(backbone)\n",
    "    freeze_bert_weights(backbone, config.freeze_layer, config.freeze_embedding, config.freeze_pool)\n",
    "elif config.model_type == 'xlnet':\n",
    "    from transformers import XLNetForSequenceClassification\n",
    "    from model import freeze_xlnet_weights\n",
    "    backbone = XLNetForSequenceClassification.from_pretrained('xlnet/xlnet-base-cased', num_labels=config.num_classes) \n",
    "    print(backbone)\n",
    "    freeze_xlnet_weights(backbone, config.freeze_layer, config.freeze_embedding, config.freeze_summary)\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init classifier\n",
    "if config.model_type == 'xlnet':\n",
    "    model = backbone\n",
    "else:\n",
    "    model = TransformerClassifier(backbone=backbone,\n",
    "                                input_size=config.input_size,\n",
    "                                hidden_layers=config.hidden_layers,\n",
    "                                num_classes=config.num_classes)\n",
    "model = model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init optimizer\n",
    "optimizer = Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset handler\n",
    "for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(filename=os.path.join(config.output_folder, 'train_log.log'), level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', filemode='w')\n",
    "\n",
    "logging.info(f'configurations: {config}')\n",
    "logging.info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练: 2024-08-02 00:23:30.800693 \n",
      "epoch 0, batch 0/45, train loss: 1.9474589824676514\n",
      "epoch 0, batch 10/45, train loss: 1.9180608987808228\n",
      "epoch 0, batch 20/45, train loss: 1.8207385540008545\n",
      "epoch 0, batch 30/45, train loss: 1.533968210220337\n",
      "epoch 0, batch 40/45, train loss: 1.5406452417373657\n",
      "梯度范数 tensor(6.0650, device='cuda:0')\n",
      "开始测试: 2024-08-02 00:23:39.570096 \n",
      "结束测试: 2024-08-02 00:23:40.920317 \n",
      "Saving models ...\n",
      "epoch 0 train loss: 1.7368096113204956, train_f1:0.07860605418682098, train acc: 0.30076760053634644\n",
      "  test loss: 1.6018669605255127, test f1: 0.061662860214710236, test acc: 0.2729805111885071\n",
      "epoch 0 best f1: 0.061662860214710236\n",
      " \n",
      "epoch 1, batch 0/45, train loss: 1.4921958446502686\n",
      "epoch 1, batch 10/45, train loss: 1.3977398872375488\n",
      "epoch 1, batch 20/45, train loss: 1.3478947877883911\n",
      "epoch 1, batch 30/45, train loss: 1.6216673851013184\n",
      "epoch 1, batch 40/45, train loss: 1.3294156789779663\n",
      "梯度范数 tensor(4.0218, device='cuda:0')\n",
      "开始测试: 2024-08-02 00:23:50.330058 \n",
      "结束测试: 2024-08-02 00:23:51.677590 \n",
      "Saving models ...\n",
      "epoch 1 train loss: 1.5069223642349243, train_f1:0.10606106370687485, train acc: 0.43893927335739136\n",
      "  test loss: 1.495082974433899, test f1: 0.07872137427330017, test acc: 0.4596100151538849\n",
      "epoch 1 best f1: 0.07872137427330017\n",
      " \n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "train_loss_lst, train_f1_lst, train_acc_lst, test_loss_lst, test_f1_lst, test_acc_lst = [], [], [], [], [], []\n",
    "print(f\"开始训练: {datetime.now()} \")\n",
    "for i in range(config.epochs):\n",
    "    total_labels_train = []\n",
    "    total_logits_train = []\n",
    "    gradnorm_list = []\n",
    "    for j, (X, y) in enumerate(train_loader):\n",
    "        for k in X.keys():\n",
    "            X[k] = X[k].to(config.device)\n",
    "        labels = y.to(config.device).to(torch.int64)\n",
    "\n",
    "        if config.model_type == 'xlnet':\n",
    "            X['labels'] = labels\n",
    "\n",
    "        # forward\n",
    "        output = model(**X)\n",
    "\n",
    "        # obtain loss and stats\n",
    "        if config.model_type == 'xlnet':\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "        else:\n",
    "            # for bert/roberta\n",
    "            logits = output\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        gradnorm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        gradnorm_list.append(gradnorm)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_labels_train.append(labels)\n",
    "            total_logits_train.append(logits)\n",
    "\n",
    "        if j % config.log_freq == 0:\n",
    "            logprint(f\"epoch {i}, batch {j}/{len(train_loader)}, train loss: {loss.item()}\")\n",
    "\n",
    "    # 打印梯度范数的均值\n",
    "    print(f'梯度范数', sum(gradnorm_list) / len(gradnorm_list))\n",
    "    # 评估训练结果\n",
    "    with torch.no_grad():\n",
    "        train_logits_all = torch.concat(total_logits_train)\n",
    "        train_labels_all = torch.concat(total_labels_train).to(torch.int64)\n",
    "        train_loss = loss_fn(train_logits_all, train_labels_all)\n",
    "        train_f1 = multiclass_f1_score(train_logits_all, train_labels_all, num_classes=8, average='macro')\n",
    "        train_acc = torch.sum(train_logits_all.argmax(-1) == train_labels_all) / len(train_labels_all)\n",
    "        train_loss_lst.append(train_loss.item())\n",
    "        train_f1_lst.append(train_f1.item())\n",
    "        train_acc_lst.append(train_acc.item())\n",
    "    print(f\"开始测试: {datetime.now()} \")\n",
    "    with torch.no_grad():\n",
    "        total_logits_test = []\n",
    "        for X, y in test_loader:\n",
    "            for k in X.keys():\n",
    "                X[k] = X[k].to(config.device)\n",
    "            labels = y.to(config.device).to(torch.int64)\n",
    "\n",
    "            if config.model_type == 'xlnet':\n",
    "                X['labels'] = labels\n",
    "\n",
    "            output = model(**X)\n",
    "\n",
    "            if config.model_type == 'xlnet':\n",
    "                logits = output.logits\n",
    "            else:\n",
    "                logits = output\n",
    "            total_logits_test.append(logits)\n",
    "\n",
    "        test_logits_all = torch.concat(total_logits_test)\n",
    "        test_labels_all = test_data.labels.to(config.device).to(torch.int64)\n",
    "\n",
    "        test_loss = loss_fn(test_logits_all, test_labels_all)\n",
    "        test_f1 = multiclass_f1_score(test_logits_all, test_labels_all, num_classes=8, average='macro')\n",
    "        test_acc = torch.sum(test_logits_all.argmax(-1) == test_labels_all) / len(test_labels_all)\n",
    "        test_loss_lst.append(test_loss.item())\n",
    "        test_f1_lst.append(test_f1.item())\n",
    "        test_acc_lst.append(test_acc.item())\n",
    "    \n",
    "    print(f\"结束测试: {datetime.now()} \")\n",
    "    if test_f1 > best_f1:\n",
    "        best_f1 = test_f1.item()\n",
    "        torch.save(model.state_dict(), os.path.join(config.output_folder, 'best_model.pt'))\n",
    "        torch.save(optimizer.state_dict(), os.path.join(config.output_folder, 'optimizer.pt'))\n",
    "\n",
    "        logprint('Saving models ...')\n",
    "\n",
    "    logprint(\n",
    "        f\"epoch {i} train loss: {train_loss}, train_f1:{train_f1}, train acc: {train_acc}\\n  test loss: {test_loss}, test f1: {test_f1}, test acc: {test_acc}\")\n",
    "    logprint(f'epoch {i} best f1: {best_f1}')\n",
    "    logprint(' ')  # empty line to separate epochs\n",
    "\n",
    "    with open(os.path.join(config.output_folder, 'results.json'), 'w') as f:\n",
    "        json.dump({\n",
    "            'train_loss': train_loss_lst,\n",
    "            'train_f1': train_f1_lst,\n",
    "            'train_acc': train_acc_lst,\n",
    "            'test_loss': test_loss_lst,\n",
    "            'test_f1': test_f1_lst,\n",
    "            'test_acc': test_acc_lst,\n",
    "        }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制 accuracy 曲线\n",
    "plot_and_save(test_f1_lst, 'Test Accuracy', config.output_folder, 'test_accuracy.png')\n",
    "\n",
    "# 绘制 test_f1_lst 曲线\n",
    "plot_and_save(test_f1_lst, 'Test F1 Score', config.output_folder, 'test_f1.png')\n",
    "\n",
    "# 绘制 test_loss_lst 曲线\n",
    "plot_and_save(test_loss_lst, 'Test Loss', config.output_folder, 'test_loss.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
